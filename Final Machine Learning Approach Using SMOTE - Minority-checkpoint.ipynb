{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection System\n",
    "\n",
    "This notebook contains my approach towards credit card fraud detection using Machine Learning Algorithms. Credit card fraud detection is a very serious modern day problem. In this notebook my aim will be to get the maximum possible accuracy for the European Tracsaction data set which was released in 2013. The metrics I will be using is F1- Score, Precision and Recall.\n",
    "\n",
    "We will first import the necessary libraries for EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, f1_score, plot_roc_curve, precision_score, recall_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us import the data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let us check if there are any null values present in our data set:**\n",
    "\n",
    "If null values are found we will replace these values with the mean or median of that data attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are no null values in our data set so we can move forward and understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA (Exploratory Data Analysis):\n",
    "In this step we will plot different graphs and try to find out different patterns in our data which will help us for feature selection.\n",
    "\n",
    "#### Distribution of Target Variable:\n",
    "The graph below shows the distribution of our target varible which has two values 0 and 1 indicating not a fraud and a fraud respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = list(df[\"Class\"].values)\n",
    "ones = data.count(1)\n",
    "zeros = data.count(0)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=[\"Not Fraud\", \"Fraud\"], y = [zeros, ones])\n",
    "plt.title(\"Fraud Transactions Vs Legit Transactions\")\n",
    "plt.xlabel(\"CLASS\")\n",
    "plt.ylabel(\"No. of Transactions\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clearly visible that there is a great imbalance in the data where we have extremely low fraud transactions and too many legit ones so to tackle this issue we will have to use techniques like undersapling the data while traning our model.\n",
    "\n",
    "### Fixing Data Set Imbalance:\n",
    "Now the next step will be data preparation for model fitting and testing. In our data as discussed above there is a lot of imbalance in the the Legit and the Fraud Transactions so we will do some oversampling or undersampling of data. As our Data set is not very huge we will use oversampling which will prevent the loss of any kind of valuable data. We will keep the fraud transactions and the legit ones in ratio of about 1:1 which will make it perfectly balanced. \n",
    "\n",
    "To do so we will take help of the imblearn library in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 284315, 1: 284315})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "over_sampler = SMOTE(sampling_strategy=\"minority\")\n",
    "X = df.iloc[:, :30]\n",
    "Y = df.iloc[:, 30]\n",
    "\n",
    "X_oversampled, Y_oversampled = over_sampler.fit_resample(X, Y)\n",
    "counter = 0\n",
    "o_df = pd.DataFrame()\n",
    "for i in list(df.columns)[:30]:\n",
    "    o_df[i] = X_oversampled[i]\n",
    "    counter += 1\n",
    "    \n",
    "o_df[\"Class\"] = Y_oversampled\n",
    "df = o_df.copy()\n",
    "Counter(Y_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let us check for the data distribution for the other attributes as well:**\n",
    "\n",
    "We want the distribution tu be Gaussian if not then we will apply data transformation tecniques to convert it into a Gaussian curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "\n",
    "def data_distribution(columns, bins = 50):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(\"Data distributions of Attributes in Data Set\")\n",
    "    counter = 1\n",
    "    for i in columns:\n",
    "        plt.subplot(3,2, counter)\n",
    "        sns.histplot(df[i], bins = bins, kde = True)\n",
    "        counter += 1\n",
    "        \n",
    "data_distribution(columns[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the graphs above it is visible that the distribution of Time, V1, V3, V4 is not gaussian so we will have to do transformation of these columns**\n",
    "1. Time = Not Smooth Gaussian\n",
    "2. V1 = Not Smooth Gaussian\n",
    "3. V3 = Left Skewed\n",
    "4. V4 = Right Skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distribution(columns[6:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some of the graphs above show normal Gaussian Distribution which means no transformation is required for those but V7, V10, V11 need transformation**\n",
    "1. V7 = Left Skewed\n",
    "2. V10 = Left Skewed\n",
    "3. V11 = Right Skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distribution(columns[12:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most of the graphs above do not show normal Gaussian Distribution which means transformation is required**\n",
    "1. V12 = Left Skewed\n",
    "2. V14 = Left Skewed\n",
    "3. V16 = Left Skewed\n",
    "4. V17 = Left Skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distribution(columns[18:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most of the graphs above show normal Gaussian Distribution except V18 which means no transformation is required for most of the columns**\n",
    "1. V18 = Left Skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distribution(columns[24:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the graph above it is seen that the Amount variable has left skewness in the Amount data and it is seen that V24 has a Gaussian Distribution but doesn't show a smooth curve so we will have to transform the data.**\n",
    "1. V24 = Not Smooth Gaussian\n",
    "\n",
    "### Transforming the Data:\n",
    "\n",
    "For transforming the data we will use the Power Transformer from sklearn. The variables to be transformed are:\n",
    "1. Time\n",
    "2. V1\n",
    "3. V3\n",
    "4. V4\n",
    "5. V7\n",
    "6. V10\n",
    "7. V11\n",
    "8. V12\n",
    "9. V14\n",
    "10. V16\n",
    "11. V17\n",
    "12. V18\n",
    "13. V24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols1 = [\"Time\", \"V1\", \"V3\", \"V4\", \"V7\", \"V10\", \"V11\"]\n",
    "cols2 = [\"V12\", \"V14\", \"V16\", \"V17\", \"V18\", \"V19\", \"V24\"]\n",
    "\n",
    "def check_best_transformer(columns):\n",
    "    pt = PowerTransformer()\n",
    "    qt = QuantileTransformer(n_quantiles=500, output_distribution='normal')\n",
    "    fig = plt.figure(figsize=(20,30))\n",
    "    plt.suptitle(\"Quantitle Vs Power Transform\")\n",
    "    j = 1\n",
    "    for i in columns:\n",
    "        array = np.array(df[i]).reshape(-1, 1)\n",
    "        y = pt.fit_transform(array)\n",
    "        x = qt.fit_transform(array)\n",
    "        plt.subplot(7,3,j)\n",
    "        sns.histplot(array, bins = 50, kde = True)\n",
    "        plt.title(f\"Original Distribution for {i}\")\n",
    "        plt.subplot(7,3,j+1)\n",
    "        sns.histplot(x, bins = 50, kde = True)\n",
    "        plt.title(f\"Quantile Transform for {i}\")\n",
    "        plt.subplot(7,3,j+2)\n",
    "        sns.histplot(y, bins = 50, kde = True)\n",
    "        plt.title(f\"Power Transform for {i}\")\n",
    "        j += 3\n",
    "\n",
    "check_best_transformer(cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_best_transformer(cols2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is clearly seen from the graphs above that Quantile Transformer works a better job at transforming the data into Gaussian Distribution. So we will apply the Quantile Tranformer on our data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(n_quantiles=500, output_distribution='normal')\n",
    "cols = cols1 + cols2\n",
    "for i in cols:\n",
    "    df[i] = qt.fit_transform(np.array(df[i]).reshape(-1,1))\n",
    "    print(f\"{i} transformation Successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Graph:\n",
    "\n",
    "Now we will check the correlation between independent variables and the target variables. To do this we will use the Pearson Correlation. the Pearson Correalation Coefficient ranges from (-1 to 1) where -1 means that the data is negatively correlated, 1 means that the data is positively correlated and 0 means that there is no correlation between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(31, 31))\n",
    "sns.heatmap(df.corr(), annot = True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the heatmap above we can see that there is a significant relation between some of the the independent variables so we might have to drop some columns from our dataset. We will keep the threshold at 0.7 and check for correlation between the columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "pairs = []\n",
    "for i in list(corr.columns)[:30]:\n",
    "    for j in list(corr.columns)[:30]:\n",
    "        if abs(corr.loc[i,j])>0.7 and corr.loc[i,j] != 1:\n",
    "            pairs.append([i, j])\n",
    "            if [i, j] not in pairs or [j, i] not in pairs:\n",
    "                print(i, ' ',j ,' ', corr.loc[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results we can observe that:\n",
    "1. V2 and V5 show strong correlation so we will drop the V5 Column.\n",
    "2. V12 and V11 are highly correlated with V14 and thus can will have quite similar effect on the target variable so we will drop th column V14.\n",
    "3. V16 V17 and V18 show a significant correlation so in this case we will drop the V17 column.\n",
    "4. V21 and V22 also show high correlation so we will drop the V22 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"V5\", \"V14\", \"V17\", \"V22\"], inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plots:\n",
    "We will plot a scatter plot to check if there are any visible clusters in our data which can indicate if we can apply clustering on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "#print(df[df[\"Class\"]==1].shape)\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(x = df[df[\"Class\"]==1][\"V1\"], y = df[df[\"Class\"]==1][\"V12\"])\n",
    "plt.title(\"Distribution of Data for Fraud Transactions\")\n",
    "plt.plot()\n",
    "plt.subplot(1,3,2)\n",
    "sns.scatterplot(x = df[df[\"Class\"]==0][\"V1\"], y = df[df[\"Class\"]==0][\"V12\"])\n",
    "plt.title(\"Distribution of Data for Legit Transactions\")\n",
    "plt.plot()\n",
    "plt.subplot(1,3,3)\n",
    "sns.scatterplot(x = df[df[\"Class\"]==0][\"V1\"], y = df[df[\"Class\"]==0][\"V12\"], color = \"red\")\n",
    "sns.scatterplot(x = df[df[\"Class\"]==1][\"V1\"], y = df[df[\"Class\"]==1][\"V12\"], color =\"green\")\n",
    "plt.title(\"Distribution of Data All Transactions\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no visible clusters in our data. It is visible that both the transactions fraud and legit are overlapping and don't show any significant distinguishable clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-Fold Cross Validation:\n",
    "\n",
    "Now to observe the performance of our models we will use the StrafiedKFold from sklearn.model_selection for this task we will split our data set into 10 folds for trainng and testing data.\n",
    "\n",
    "The models that we will test are:\n",
    "\n",
    "1. Random Forest\n",
    "2. Logistic Regression\n",
    "3. Naive Bayes\n",
    "4. Stochastic Gradient Descent Classifier\n",
    "5. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :26].values\n",
    "Y = df.iloc[:, 26].values\n",
    "\n",
    "def classification_evaluation(classifier, X, Y, classifier_name, columns):\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    accuracy_arr = np.zeros(shape=(1,10))\n",
    "    f1_score_arr = np.zeros(shape=(1,10))\n",
    "    precision_arr = np.zeros(shape=(1,10))\n",
    "    recall_arr = np.zeros(shape=(1,10))\n",
    "    feature_importance_arr = np.zeros(shape=(1,26))\n",
    "    time_arr = []\n",
    "    \n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    start_time = time.time()\n",
    "    for i, (train, test) in enumerate(cv.split(X, Y)):\n",
    "        itr_start_time = time.time()\n",
    "        classifier.fit(X[train], Y[train])\n",
    "        itr_end_time = time.time()\n",
    "        viz = plot_roc_curve(classifier, X[test], Y[test],\n",
    "                             name=f'ROC fold {i}',\n",
    "                             alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "        \n",
    "        try:\n",
    "            feature_importance_arr += classifier.feature_importances_\n",
    "        except:\n",
    "            try:\n",
    "                feature_importance_arr += np.array([abs(i) for i in classifier.coef_[0]])\n",
    "            except:\n",
    "                feature_importance_arr += np.array([abs(i) for i in permutation_importance(classifier, X[test], Y[test]).importances_mean])\n",
    "            \n",
    "        \n",
    "        predict = classifier.predict(X[test])\n",
    "        accuracy = classifier.score(X[test], Y[test])\n",
    "        accuracy_arr[0, i] = accuracy\n",
    "        f1 = f1_score(predict, Y[test])\n",
    "        f1_score_arr[0, i] = f1\n",
    "        precision = precision_score(predict, Y[test])\n",
    "        precision_arr[0, i] = precision\n",
    "        recall = recall_score(predict, Y[test])\n",
    "        recall_arr[0, i] = recall\n",
    "        train, test = train, test\n",
    "        time_arr.append(itr_end_time-itr_start_time)\n",
    "        print(f\"Fold-{i}:  accuracy: {accuracy}  f1-score: {f1}  precision: {precision}  recall: {recall}  time: {itr_end_time-itr_start_time}\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print()\n",
    "    print(f\"Classification Report for {classifier_name}\")\n",
    "    print(\"Average Accuracy :\", np.mean(accuracy_arr))\n",
    "    print(\"Average F1-Score :\", np.mean(f1_score_arr))\n",
    "    print(\"Average Precision :\", np.mean(precision_arr))\n",
    "    print(\"Average Recall :\", np.mean(recall_arr))\n",
    "    print(\"Total Time :\", end_time - start_time)\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "           title=f\"Receiver Operating Characteristic for {classifier_name}\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    feature_importance_arr /= 10\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    feature_imp = pd.Series(feature_importance_arr[0], index = columns[:26])\n",
    "    feature_imp.nlargest(10).plot(kind = 'bar')\n",
    "    plt.title(f\"Feature Importance of {classifier_name}\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(time_arr, 'go-', color=\"darkblue\")\n",
    "    plt.plot([max(time_arr) for i in range(10)], color='lightcoral', label='UL')\n",
    "    plt.plot([min(time_arr) for i in range(10)], color='lightgreen', label='LL')\n",
    "    plt.title(\"Training time for every iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Time in sec.\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean(accuracy_arr), np.mean(f1_score_arr), np.mean(precision_arr), np.mean(recall_arr), mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(max_iter=1000)\n",
    "lr_acc, lr_f1, lr_precision, lr_recall, lr_auc = classification_evaluation(classifier, X, Y, \"Logistic Regression\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SGDClassifier(loss=\"modified_huber\", penalty = \"l2\")\n",
    "sgd_acc, sgd_f1, sgd_precision, sgd_recall, sgd_auc = classification_evaluation(classifier, X, Y, \"Stochastic Gradient Descent Classifier\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianNB()\n",
    "gn_acc, gn_f1, gn_precision, gn_recall, gn_auc = classification_evaluation(classifier, X, Y, \"Gaussian Naive Bayes\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=20, max_depth = 26)\n",
    "rf_acc, rf_f1, rf_precision, rf_recall, rf_auc = classification_evaluation(classifier, X, Y, \"Random Forest Classifier\", list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(use_label_encoder=False, disable_default_eval_metric=1)\n",
    "xg_acc, xg_f1, xg_precision, xg_recall, xg_auc = classification_evaluation(classifier, X, Y, \"XGBoost\", list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model Creation:\n",
    "\n",
    "In this hybrid model we will try to use the weighted average of the probabilities of all the models which clear the threshold and compare it with the result of individual predictions. It is believed that a hybrid model is bound to give better or more generalized results as any models having a bias will be neutralized in this process. \n",
    "\n",
    "Threshold:\n",
    "1. Accuracy > 90\n",
    "2. f1 > 90\n",
    "3. auc > 90\n",
    "\n",
    "This hybrid model will be applied for both RandomOverSampling as well as SMOTEK over sampling. This Notebook contains the results for Random Oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [lr_acc, sgd_acc, gn_acc, rf_acc, xg_acc]\n",
    "f1_scores = [lr_f1, sgd_f1, gn_f1, rf_f1, xg_f1]\n",
    "auc_scores = [lr_auc, sgd_auc, gn_auc, rf_auc, xg_auc]\n",
    "\n",
    "mean_acc = sum(accuracy)/5\n",
    "mean_f1 = sum(f1_scores)/5\n",
    "mean_auc = sum(auc_scores)/5\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.title(\"Metric Comparisions of the tested Classifiers\")\n",
    "plt.plot(accuracy,'go-', color=\"blue\", label=\"Accuracy\")\n",
    "plt.plot(f1_scores,'go-', color=\"red\", label=\"F1-Score\")\n",
    "plt.plot(auc_scores,'go-', color=\"green\", label=\"AUC Score\")\n",
    "plt.xticks([i for i in range(5)], [\"LR\", \"SGD\", \"GNB\", \"RF\", \"XGBC\"])\n",
    "plt.legend()\n",
    "plt.xlabel('Model Names')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "plt.title(\"Mean Metric Scores\")\n",
    "plt.bar(height=[mean_acc, mean_f1, mean_auc], x = [\"Accuracy\", \"F1 Score\", \"AUC\"], width=.3, color=[\"lightblue\", \"red\", \"green\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_model_formation(clf1, clf2, clf3, clf4, clf5, X, Y, columns):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=1)\n",
    "    \n",
    "    print(\"Phase 1 Started\")\n",
    "    print(\"Training Models and Generating Report:\")\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")\n",
    "    try:\n",
    "        t1 = time.time()\n",
    "        clf1.fit(x_train, y_train)\n",
    "        clf1_acc = clf1.score(x_test, y_test)\n",
    "        pred1 = clf1.predict(x_test)\n",
    "        clf1_f1 = f1_score(pred1, y_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, clf1.predict_proba(x_test)[:, 1])\n",
    "        clf1_auc = metrics.auc(fpr, tpr)\n",
    "        feature_imp1 = pd.Series([abs(i) for i in clf1.coef_[0]], index = columns[:26])\n",
    "        t1_ = time.time()\n",
    "        final_t1 = t1_ - t1\n",
    "        print(\"Model 1 Trained and Tested: \")\n",
    "        print(\"Accuracy: \", clf1_acc)\n",
    "        print(\"F1 Score:\", clf1_f1)\n",
    "        print(\"AUC Score: \", clf1_auc)\n",
    "        print(\"Time :\", final_t1)\n",
    "        \n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        t2 = time.time()\n",
    "        clf2.fit(x_train, y_train)\n",
    "        clf2_acc = clf2.score(x_test, y_test)\n",
    "        pred2 = clf2.predict(x_test)\n",
    "        clf2_f1 = f1_score(pred2, y_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, clf2.predict_proba(x_test)[:, 1])\n",
    "        clf2_auc = metrics.auc(fpr, tpr)\n",
    "        feature_imp2 = pd.Series([abs(i) for i in clf2.coef_[0]], index = columns[:26])\n",
    "        t2_ = time.time()\n",
    "        final_t2 = t2_ - t2\n",
    "        print(\"Model 2 Trained and Tested: \")\n",
    "        print(\"Accuracy: \", clf2_acc)\n",
    "        print(\"F1 Score:\", clf2_f1)\n",
    "        print(\"AUC Score: \", clf2_auc)\n",
    "        print(\"Time :\", final_t2)\n",
    "        \n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        t3 = time.time()\n",
    "        clf3.fit(x_train, y_train)\n",
    "        clf3_acc = clf3.score(x_test, y_test)\n",
    "        pred3 = clf3.predict(x_test)\n",
    "        clf3_f1 = f1_score(pred3, y_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, clf3.predict_proba(x_test)[:, 1])\n",
    "        clf3_auc = metrics.auc(fpr, tpr)\n",
    "        feature_imp3 = pd.Series([abs(i) for i in permutation_importance(clf3, x_test, y_test).importances_mean], index = columns[:26])\n",
    "        t3_ = time.time()\n",
    "        final_t3 = t3_ - t3\n",
    "        print(\"Model 3 Trained and Tested: \")\n",
    "        print(\"Accuracy: \", clf3_acc)\n",
    "        print(\"F1 Score:\", clf3_f1)\n",
    "        print(\"AUC Score: \", clf3_auc)\n",
    "        print(\"Time :\", final_t3)\n",
    "        \n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        t4 = time.time()\n",
    "        clf4.fit(x_train, y_train)\n",
    "        clf4_acc = clf4.score(x_test, y_test)\n",
    "        pred4 = clf4.predict(x_test)\n",
    "        clf4_f1 = f1_score(pred4, y_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, clf4.predict_proba(x_test)[:, 1])\n",
    "        clf4_auc = metrics.auc(fpr, tpr)\n",
    "        feature_imp4 = pd.Series(clf4.feature_importances_, index = columns[:26])\n",
    "        t4_ = time.time()\n",
    "        final_t4 = t4_ - t4\n",
    "        print(\"Model 4 Trained and Tested: \")\n",
    "        print(\"Accuracy: \", clf4_acc)\n",
    "        print(\"F1 Score:\", clf4_f1)\n",
    "        print(\"AUC Score: \", clf4_auc)\n",
    "        print(\"Time :\", final_t4)\n",
    "        \n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        t5 = time.time()\n",
    "        clf5.fit(x_train, y_train)\n",
    "        clf5_acc = clf5.score(x_test, y_test)\n",
    "        pred5 = clf5.predict(x_test)\n",
    "        clf5_f1 = f1_score(pred5, y_test)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test, clf5.predict_proba(x_test)[:, 1])\n",
    "        clf5_auc = metrics.auc(fpr, tpr)\n",
    "        feature_imp5 = pd.Series(clf5.feature_importances_, index = columns[:26])\n",
    "        t5_ = time.time()\n",
    "        final_t5 = t5_ - t5\n",
    "        print(\"Model 5 Trained and Tested: \")\n",
    "        print(\"Accuracy: \", clf5_acc)\n",
    "        print(\"F1 Score:\", clf5_f1)\n",
    "        print(\"AUC Score: \", clf5_auc)\n",
    "        print(\"Time :\", final_t5)\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Process Successfully completed without any errors....\")\n",
    "    except:\n",
    "        print(\"Error Occured during model training and evaluation....\")\n",
    "        return \"Error!!\"\n",
    "    \n",
    "    print()\n",
    "    print(\"Phase 2 Started\")\n",
    "    print(\"The slelection process has started....\")\n",
    "    all_classifiers = [clf1, clf2, clf3, clf4, clf5]\n",
    "    all_classifiers_names = [\"clf1\", \"clf2\", \"clf3\", \"clf4\", \"clf5\"]\n",
    "    selected_classifiers = [0 for i in range(5)]\n",
    "    classifiers = {\"clf1\":clf1, \"clf2\":clf2, \"clf3\":clf3, \"clf4\":clf4, \"clf5\":clf5}\n",
    "    accuracys = [clf1_acc, clf2_acc, clf3_acc, clf4_acc, clf5_acc]\n",
    "    f1_scores = [clf1_f1, clf2_f1, clf3_f1, clf4_f1, clf5_f1]\n",
    "    auc_scores = [clf1_auc, clf2_auc, clf3_auc, clf4_auc, clf5_auc]\n",
    "    train_time = [final_t1, final_t2, final_t3, final_t4, final_t5]\n",
    "    f_imp = [feature_imp1, feature_imp2, feature_imp3, feature_imp4, feature_imp5]\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for o in range(5):\n",
    "        plt.subplot(3,2,o+1)\n",
    "        f_imp[o].nlargest(10).plot(kind = 'barh')\n",
    "        plt.title(f\"Feature Importance of {all_classifiers_names[o]}\")\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    plt.plot([i*100 for i in accuracys], 'go-', color='lightblue', label='Accuracy')\n",
    "    plt.plot([i*100 for i in f1_scores], 'go-', color='red', label='F1-Score')\n",
    "    plt.plot([i*100 for i in auc_scores], 'go-', color='green', label='AUC-Score')\n",
    "    plt.plot(train_time, 'go-', color='darkblue', label='Train Time(sec)')\n",
    "    plt.title('Model Performance')\n",
    "    plt.legend()\n",
    "    plt.xticks([i for i in range(5)], all_classifiers_names)\n",
    "    plt.xlabel('Model Names')\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(5):\n",
    "        if accuracys[i] >= .90 and f1_scores[i] >= .90 and auc_scores[i] >= .90:\n",
    "            selected_classifiers[i] = 1\n",
    "            \n",
    "    for j in range(5):\n",
    "        if not selected_classifiers[j]:\n",
    "            all_classifiers[j] = None\n",
    "            all_classifiers_names[j] = None\n",
    "            f1_scores[j] = None\n",
    "            \n",
    "    all_classifiers = [i for i in all_classifiers if i != None]\n",
    "    all_classifiers_names = [i for i in all_classifiers_names if i != None]\n",
    "    f1_scores = [i for i in f1_scores if i != None]\n",
    "    \n",
    "    print(\"The selected classifiers are: \")\n",
    "    for i in range(len(all_classifiers)):\n",
    "        print(f\"{i+1}. {all_classifiers_names[i]}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Phase 3 Started\")\n",
    "    # Final testing with formula:\n",
    "    numerator = []\n",
    "    denominator = sum([i for i in f1_scores])\n",
    "    for i in range(len(all_classifiers)):\n",
    "        proba = all_classifiers[i].predict_proba(x_test)[:, 1]\n",
    "        # print(proba)\n",
    "        final_num = np.array([x*f1_scores[i] for x in proba])\n",
    "        numerator.append(final_num)\n",
    "        \n",
    "    numerator_val = np.zeros(shape=y_test.shape)\n",
    "    for j in range(len(numerator)):\n",
    "        numerator_val += numerator[j]\n",
    "        \n",
    "    numerator_val /= denominator\n",
    "    \n",
    "    for i in range(len(numerator_val)):\n",
    "        if numerator_val[i] > 0.5:\n",
    "            numerator_val[i] = 1\n",
    "        else:\n",
    "            numerator_val[i] = 0\n",
    "    \n",
    "    print(\"The final model is ready and has been evaluated the results are as following:\")\n",
    "    print(\"Accuracy :\", metrics.accuracy_score(numerator_val, y_test))\n",
    "    print(\"F1 Score :\", f1_score(numerator_val, y_test))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, numerator_val)\n",
    "    print(\"AUC Score: \", metrics.auc(fpr, tpr))\n",
    "    print(\"Compilation successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model_formation(LogisticRegression(max_iter=1000), SGDClassifier(loss=\"modified_huber\", penalty = \"l2\"), \n",
    "                      GaussianNB(), RandomForestClassifier(n_estimators=20, max_depth = 26), \n",
    "                       XGBClassifier(use_label_encoder=False, disable_default_eval_metric=1), X, Y, list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
